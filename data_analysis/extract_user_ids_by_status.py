#!/usr/bin/env python3
"""
ç”¨æˆ·IDæŠ½å–è„šæœ¬

ä»è®¢å•CSVæ–‡ä»¶ä¸­æ ¹æ®è®¢å•çŠ¶æ€æŠ½å–æ­£å¸¸çŠ¶æ€çš„user_idï¼Œ
ä»é»‘åå•CSVæ–‡ä»¶ä¸­æŠ½å–é»‘åå•ç”¨æˆ·uidï¼Œ
å¹¶ç¡®ä¿æ­£å¸¸ç”¨æˆ·é›†åˆä¸­ä¸åŒ…å«é»‘åå•ç”¨æˆ·ï¼ˆå»é‡å¤„ç†ï¼‰ã€‚

è¾“å‡ºæ–‡ä»¶ï¼š
- normal_user_ids.txt: æ­£å¸¸çŠ¶æ€è®¢å•çš„ç”¨æˆ·IDï¼ˆå·²æ’é™¤é»‘åå•ç”¨æˆ·ï¼‰
- blacklist_user_ids.txt: é»‘åå•ç”¨æˆ·UID
- overlap_user_ids.txt: æ—¢åœ¨æ­£å¸¸è®¢å•åˆåœ¨é»‘åå•ä¸­çš„é‡å ç”¨æˆ·IDï¼ˆä¾›åç»­åˆ†æï¼‰

ç”¨äºäºŒåˆ†ç±»ä»»åŠ¡çš„è®­ç»ƒæ•°æ®å‡†å¤‡ã€‚
"""

from __future__ import annotations
import pandas as pd
from pathlib import Path
from typing import Set, List
import argparse

# è®¢å•çŠ¶æ€åˆ†ç±»å®šä¹‰
EXCLUDED_STATUSES = {
    "INIT", "INITLOAN", "VERIFCANCEL", "PREVERIF", "LOAN_REFUSE",
    "ON_ROUTE", "HOLD_ON", "RISK_CONFIRM", "PRESIGN", "SINGFAIL",
    "FINANCING", "CANCEL", "PRELOAN", "FAIL", "REPAYING", "LOAN_SUCESS_PRE_WP", "EARLYREPAYING", "REFUNDSETTLED", "WAITING_PAYMENT", "WAITING_WITHDRAW",
    "WITHDRAWING", "WITHDRAW_SUCESS", "SETTLED", "NOTICE_SETTLE", "WITHDRAW_FAIL_CARD"
}

NORMAL_STATUSES = {
    "EARLY_REPAYED",
    "REPAYED"
}

ABNORMAL_STATUSES = {
    "OVERDUE"
}


def load_order_data(csv_path: str) -> pd.DataFrame:
    """åŠ è½½è®¢å•CSVæ•°æ®"""
    try:
        df = pd.read_csv(
            csv_path,
            dtype=str,
            keep_default_na=False,
            na_filter=False,
            encoding="utf-8-sig"
        )
        print(f"âœ… æˆåŠŸåŠ è½½è®¢å•æ•°æ®ï¼Œå…± {len(df)} æ¡è®°å½•")
        return df
    except Exception as e:
        print(f"âŒ åŠ è½½CSVæ–‡ä»¶å¤±è´¥: {e}")
        raise


def extract_user_ids_by_status(df: pd.DataFrame, status_set: Set[str]) -> List[str]:
    """æ ¹æ®è®¢å•çŠ¶æ€é›†åˆæŠ½å–ç”¨æˆ·ID"""
    # ç­›é€‰ç¬¦åˆçŠ¶æ€çš„è®¢å•
    filtered_df = df[df['order_status'].isin(status_set)]
    
    # è·å–å»é‡çš„ç”¨æˆ·IDåˆ—è¡¨
    user_ids = filtered_df['user_id'].unique().tolist()
    
    # è¿‡æ»¤æ‰ç©ºå€¼
    user_ids = [uid for uid in user_ids if uid and uid.strip()]
    
    return sorted(user_ids)


def save_user_ids_to_file(user_ids: List[str], output_path: str) -> None:
    """å°†ç”¨æˆ·IDåˆ—è¡¨ä¿å­˜åˆ°æ–‡ä»¶"""
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            for uid in user_ids:
                f.write(f"{uid}\n")
        print(f"âœ… æˆåŠŸä¿å­˜ {len(user_ids)} ä¸ªç”¨æˆ·IDåˆ°: {output_path}")
    except Exception as e:
        print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
        raise


def analyze_order_status_distribution(df: pd.DataFrame) -> None:
    """åˆ†æè®¢å•çŠ¶æ€åˆ†å¸ƒ"""
    print(f"\n{'='*60}")
    print("è®¢å•çŠ¶æ€åˆ†å¸ƒåˆ†æ")
    print(f"{'='*60}")
    
    status_counts = df['order_status'].value_counts()
    
    print(f"æ€»è®¢å•æ•°: {len(df)}")
    print(f"ä¸åŒçŠ¶æ€æ•°: {len(status_counts)}")
    
    print(f"\nçŠ¶æ€åˆ†å¸ƒç»Ÿè®¡:")
    for status, count in status_counts.items():
        percentage = count / len(df) * 100
        
        if status in NORMAL_STATUSES:
            category = "æ­£å¸¸"
        elif status in ABNORMAL_STATUSES:
            category = "å¼‚å¸¸"
        elif status in EXCLUDED_STATUSES:
            category = "æ’é™¤"
        else:
            category = "æœªåˆ†ç±»"
        
        print(f"  {status:<20} {count:>6} ({percentage:>5.1f}%) [{category}]")
    
    # ç»Ÿè®¡å„åˆ†ç±»çš„è®¢å•æ•°
    normal_count = len(df[df['order_status'].isin(NORMAL_STATUSES)])
    abnormal_count = len(df[df['order_status'].isin(ABNORMAL_STATUSES)])
    excluded_count = len(df[df['order_status'].isin(EXCLUDED_STATUSES)])
    unclassified_count = len(df) - normal_count - abnormal_count - excluded_count
    
    print(f"\nåˆ†ç±»æ±‡æ€»:")
    print(f"  æ­£å¸¸çŠ¶æ€è®¢å•: {normal_count} ({normal_count/len(df)*100:.1f}%)")
    print(f"  å¼‚å¸¸çŠ¶æ€è®¢å•: {abnormal_count} ({abnormal_count/len(df)*100:.1f}%)")
    print(f"  æ’é™¤çŠ¶æ€è®¢å•: {excluded_count} ({excluded_count/len(df)*100:.1f}%)")
    if unclassified_count > 0:
        print(f"  æœªåˆ†ç±»çŠ¶æ€è®¢å•: {unclassified_count} ({unclassified_count/len(df)*100:.1f}%)")


def load_blacklist_data(csv_path: str) -> pd.DataFrame:
    """åŠ è½½é»‘åå•CSVæ•°æ®"""
    try:
        df = pd.read_csv(
            csv_path,
            dtype=str,
            keep_default_na=False,
            na_filter=False,
            encoding="utf-8-sig"
        )
        print(f"âœ… æˆåŠŸåŠ è½½é»‘åå•æ•°æ®ï¼Œå…± {len(df)} æ¡è®°å½•")
        return df
    except Exception as e:
        print(f"âŒ åŠ è½½é»‘åå•CSVæ–‡ä»¶å¤±è´¥: {e}")
        raise


def extract_blacklist_uids(df: pd.DataFrame) -> List[str]:
    """ä»é»‘åå•æ•°æ®ä¸­æŠ½å–UID"""
    # è·å–å»é‡çš„UIDåˆ—è¡¨ï¼ˆä½¿ç”¨idå­—æ®µï¼‰
    uids = df['id'].unique().tolist()
    
    # è¿‡æ»¤æ‰ç©ºå€¼
    uids = [uid for uid in uids if uid and uid.strip()]
    
    return sorted(uids)


def main():
    parser = argparse.ArgumentParser(description="ä»è®¢å•å’Œé»‘åå•CSVæ–‡ä»¶ä¸­æŠ½å–ç”¨æˆ·ID")
    parser.add_argument("--order-input", "-oi", type=str, default="data/è®¢å•orderä¿¡æ¯.csv", 
                       help="è®¢å•CSVæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--blacklist-input", "-bi", type=str, default="data/é»‘åå•.csv", 
                       help="é»‘åå•CSVæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output-dir", "-o", type=str, default="data_analysis", 
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--analyze-only", action="store_true", 
                       help="ä»…åˆ†æçŠ¶æ€åˆ†å¸ƒï¼Œä¸ç”Ÿæˆè¾“å‡ºæ–‡ä»¶")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    order_path = Path(args.order_input)
    blacklist_path = Path(args.blacklist_input)
    
    if not order_path.exists():
        print(f"âŒ è®¢å•æ–‡ä»¶ä¸å­˜åœ¨: {order_path}")
        return
    
    if not blacklist_path.exists():
        print(f"âŒ é»‘åå•æ–‡ä»¶ä¸å­˜åœ¨: {blacklist_path}")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)
    
    # åŠ è½½è®¢å•æ•°æ®
    print("åŠ è½½è®¢å•æ•°æ®...")
    order_df = load_order_data(str(order_path))
    
    # åˆ†æçŠ¶æ€åˆ†å¸ƒ
    analyze_order_status_distribution(order_df)
    
    # åŠ è½½é»‘åå•æ•°æ®
    print("\nåŠ è½½é»‘åå•æ•°æ®...")
    blacklist_df = load_blacklist_data(str(blacklist_path))
    
    if args.analyze_only:
        print("\nâ¹ï¸  ä»…åˆ†ææ¨¡å¼ï¼Œä¸ç”Ÿæˆè¾“å‡ºæ–‡ä»¶")
        return
    
    # æŠ½å–æ­£å¸¸çŠ¶æ€ç”¨æˆ·ID
    print(f"\n{'='*60}")
    print("æŠ½å–ç”¨æˆ·ID")
    print(f"{'='*60}")
    
    normal_user_ids = extract_user_ids_by_status(order_df, NORMAL_STATUSES)
    print(f"âœ… ä»è®¢å•ä¸­æŠ½å–æ­£å¸¸çŠ¶æ€ç”¨æˆ·: {len(normal_user_ids)} ä¸ª")
    
    # æŠ½å–é»‘åå•ç”¨æˆ·UID
    blacklist_uids = extract_blacklist_uids(blacklist_df)
    print(f"âœ… ä»é»‘åå•ä¸­æŠ½å–ç”¨æˆ·UID: {len(blacklist_uids)} ä¸ª")
    
    # æ£€æŸ¥é‡å å¹¶ç§»é™¤é»‘åå•ç”¨æˆ·
    blacklist_set = set(blacklist_uids)
    normal_set = set(normal_user_ids)
    original_normal_count = len(normal_user_ids)
    
    # æ‰¾å‡ºé‡å çš„ç”¨æˆ·ID
    overlap_user_ids = sorted(list(normal_set & blacklist_set))
    
    # ä»æ­£å¸¸ç”¨æˆ·ä¸­ç§»é™¤é»‘åå•ç”¨æˆ·
    clean_normal_user_ids = [uid for uid in normal_user_ids if uid not in blacklist_set]
    removed_count = original_normal_count - len(clean_normal_user_ids)
    
    if removed_count > 0:
        print(f"ğŸ”„ ä»æ­£å¸¸ç”¨æˆ·ä¸­ç§»é™¤äº† {removed_count} ä¸ªé»‘åå•ç”¨æˆ·")
        print(f"ğŸ“Š å‘ç° {len(overlap_user_ids)} ä¸ªé‡å ç”¨æˆ·ï¼ˆæ—¢åœ¨æ­£å¸¸è®¢å•åˆåœ¨é»‘åå•ä¸­ï¼‰")
    
    # ä¿å­˜æ–‡ä»¶
    normal_output_path = output_dir / "normal_user_ids.txt"
    save_user_ids_to_file(clean_normal_user_ids, str(normal_output_path))
    
    blacklist_output_path = output_dir / "blacklist_user_ids.txt"
    save_user_ids_to_file(blacklist_uids, str(blacklist_output_path))
    
    # ä¿å­˜é‡å ç”¨æˆ·ID
    if overlap_user_ids:
        overlap_output_path = output_dir / "overlap_user_ids.txt"
        save_user_ids_to_file(overlap_user_ids, str(overlap_output_path))
        print(f"ğŸ“‹ é‡å ç”¨æˆ·IDå·²ä¿å­˜åˆ°: {overlap_output_path}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    print(f"\nâœ… æŠ½å–å®Œæˆ!")
    print(f"æ­£å¸¸çŠ¶æ€ç”¨æˆ·æ•°ï¼ˆå·²å»é‡é»‘åå•ï¼‰: {len(clean_normal_user_ids)}")
    print(f"é»‘åå•ç”¨æˆ·æ•°: {len(blacklist_uids)}")
    print(f"é‡å ç”¨æˆ·æ•°: {len(overlap_user_ids)}")
    print(f"æ€»è®¡å¯ç”¨äºåˆ†ç±»çš„ç”¨æˆ·æ•°: {len(clean_normal_user_ids) + len(blacklist_uids)}")
    
    # æ˜¾ç¤ºé‡å ç”¨æˆ·çš„è¯¦ç»†ä¿¡æ¯
    if overlap_user_ids:
        print(f"\nğŸ” é‡å ç”¨æˆ·åˆ†æ:")
        print(f"  - è¿™äº›ç”¨æˆ·æ—¢æœ‰æ­£å¸¸è¿˜æ¬¾è®¢å•ï¼Œåˆåœ¨é»‘åå•ä¸­")
        print(f"  - å¯èƒ½è¡¨ç¤ºç”¨æˆ·è¡Œä¸ºå˜åŒ–æˆ–æ•°æ®è´¨é‡é—®é¢˜")
        print(f"  - å‰5ä¸ªé‡å ç”¨æˆ·ç¤ºä¾‹: {overlap_user_ids[:5]}")
        print(f"  - é‡å ç”¨æˆ·å æ­£å¸¸ç”¨æˆ·æ¯”ä¾‹: {len(overlap_user_ids)/original_normal_count*100:.2f}%")
        print(f"  - é‡å ç”¨æˆ·å é»‘åå•æ¯”ä¾‹: {len(overlap_user_ids)/len(blacklist_uids)*100:.2f}%")
    
    # æ£€æŸ¥æœªåˆ†ç±»çŠ¶æ€
    all_statuses = set(order_df['order_status'].unique())
    classified_statuses = NORMAL_STATUSES | ABNORMAL_STATUSES | EXCLUDED_STATUSES
    unclassified_statuses = all_statuses - classified_statuses
    
    if unclassified_statuses:
        print(f"âš ï¸  å‘ç°æœªåˆ†ç±»çš„è®¢å•çŠ¶æ€: {unclassified_statuses}")
        print("è¯·æ£€æŸ¥çŠ¶æ€å®šä¹‰æ˜¯å¦å®Œæ•´")


if __name__ == "__main__":
    main()